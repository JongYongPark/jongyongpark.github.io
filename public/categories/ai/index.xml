<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AI on RyanLabs</title><link>https://jongyongpark.github.io/categories/ai/</link><description>Recent content in AI on RyanLabs</description><generator>Hugo -- 0.142.0</generator><language>ko</language><lastBuildDate>Wed, 19 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://jongyongpark.github.io/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>OpenWebUI</title><link>https://jongyongpark.github.io/posts/ai/openwebui/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://jongyongpark.github.io/posts/ai/openwebui/</guid><description>&lt;h1 id="내가-설치한-방법">내가 설치한 방법&lt;/h1>
&lt;p>pip install open-webui
open-webui serve &amp;ndash;port 8180&lt;/p>
&lt;h1 id="openwebui-내-설정">OpenWebUI 내 설정&lt;/h1>
&lt;p>Web browser URL: http://jonpark-ec2:8180/admin/settings
OpenWebUI 는 ec2에 설치되어 있으므로 http://localhost:11434 를 사용한다.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>OpenAI API
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Manage OpenAI API Connections
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>https://api.openai.com/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>API Key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Ollama API
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Manage Ollama API Connections
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>http://localhost:11434
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Trouble accessing Ollama? Click here for help.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Direct Connections
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Direct Connections allow users to connect to their own OpenAI compatible API endpoints.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h1 id="msty-에서-ec2-에-설치된-ollama-접속하기">Msty 에서 ec2 에 설치된 Ollama 접속하기&lt;/h1>
&lt;p>사용하지 말것것.
모든 것을 API key를 통해 처리함.
ChatGPT 경우에도 API key 를 사용하면 별도의 요금이 차감됨.&lt;/p></description></item><item><title>Kiwoom</title><link>https://jongyongpark.github.io/posts/ai/kiwoon/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://jongyongpark.github.io/posts/ai/kiwoon/</guid><description/></item><item><title>Ollama</title><link>https://jongyongpark.github.io/posts/ai/ollama/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://jongyongpark.github.io/posts/ai/ollama/</guid><description>&lt;h1 id="ec2의-ubuntu-인스턴스에서-ollama를-설치하고-사용하는-방법">EC2의 Ubuntu 인스턴스에서 Ollama를 설치하고 사용하는 방법&lt;/h1>
&lt;p>EC2의 Ubuntu 인스턴스에서 Ollama를 설치하고 사용하는 방법을 안내해드리겠습니다.&lt;/p>
&lt;h3 id="1-ollama-설치">1. Ollama 설치&lt;/h3>
&lt;p>Ollama는 로컬 환경에서 대규모 언어 모델(LLM)을 실행할 수 있게 해주는 오픈 소스 프레임워크입니다. Ubuntu에서 Ollama를 설치하려면 다음 단계를 따르세요.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>시스템 업데이트 및 필수 패키지 설치:&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo apt update &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> sudo apt upgrade -y
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo apt install -y curl
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ollama 설치 스크립트 실행:&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -fsSL https://ollama.com/install.sh | sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;strong>설치 확인:&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>ollama --version
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>위 명령어를 실행하여 Ollama가 정상적으로 설치되었는지 확인합니다.&lt;/p></description></item></channel></rss>